# -*- coding: utf-8 -*-
"""ML_Dicoding 2 (Advance).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n08LRR5EAoRitdkv_3IxmE3DTrHYdkId

### **Nama: Muhammad Amanda**
### **Username: muh_amanda** 
### **Kota: Kota Jakarta Timur, DKI Jakarta**
### *Waktu bergabung pada 12 May 2020*

# Proyek Kedua : Membuat Model Machine Learning dengan Data Time Series
"""

from google.colab import drive
drive.mount('/content/drive')

import os
os.environ['KAGGLE_CONFIG_DIR'] = "/content"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content

!kaggle datasets download -d sumanthvrao/daily-climate-time-series-data

!ls

#unzipping the zip files and deleting the zip files
!unzip \*.zip  && rm *.zip

"""# **Import library**"""

import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf

"""# **Load dataset**

* Data: [Daily Climate time series data](https://www.kaggle.com/sumanthvrao/daily-climate-time-series-data?select=DailyDelhiClimateTrain.csv)
* 1600 observation
* 
"""

df = pd.read_csv('DailyDelhiClimateTrain.csv')
data_train = df[["date","wind_speed"]]
data_train.info()
data_train.head()
df.describe()

data_train.isnull().sum() #cek data hilang

## mengubah nilai-nilai dari dataframe ke dalam tipe data numpy array
dates = data_train['date'].values
temp  = data_train['wind_speed'].values

"""# **Split Dataset**

membagi dataset menjadi data training (80%) dan data test (20%).
"""

from sklearn.model_selection import train_test_split
train_dates, test_dates, train_temp, test_temp = train_test_split(dates, temp, test_size=0.2)

"""# **Plot Data**

melakuklan plot data training.
"""

plt.figure(figsize=(15,5))
plt.plot(train_dates, train_temp)
plt.title('Temperature average',
          fontsize=20);

"""# **Arsitektur model**
* menggunakan LSTM.
* menggunakan model sequential.
* menggunakan Learning Rate pada Optimizer.
"""

# Fungsi di bawah menerima sebuah series/atribut kita yang telah di konversi menjadi tipe numpy, lalu mengembalikan label dan atribut dari dataset dalam bentuk batch.

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)


train_set = windowed_dataset(train_temp, window_size=100, batch_size=100, shuffle_buffer=1000)

model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(64, return_sequences=True),
  tf.keras.layers.LSTM(64),
  tf.keras.layers.Dense(128, activation="relu"),
  tf.keras.layers.Dense(64, activation="relu"),
  tf.keras.layers.Dense(1),
])

# arsitektur model gunakan 2 buah layer LSTM
# layer pertama LSTM harus memiliki parameter return_sequences yang bernilai True

optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)

"""# **Skala data**
* 10% skala data training
* 10% skala data testing
"""

(data_train['wind_speed'].max()-data_train['wind_speed'].min())*0.1

(train_temp.max()-train_temp.min())*0.1

(test_temp.max()-test_temp.min())*0.1

"""# **Metric evaluasi dan fitting model**"""

model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"]) # Loss function Huber umum digunakan pada kasus time series.
#history = model.fit(temp,epochs=30, batch_size=100, verbose=2)

history = model.fit(train_set, epochs=100, batch_size=100, verbose=2)

"""# **Plot mae dan loss**"""

import matplotlib.pyplot as plt
plt.plot(history.history['mae'])
plt.title('Akurasi Model')
plt.ylabel('mae')
plt.xlabel('epoch')
plt.show()

plt.plot(history.history['loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.show()

"""# **Hasil evaluasi model pada data test**"""

test_set = windowed_dataset(test_temp, window_size=100, batch_size=100, shuffle_buffer=1000)
history_test = model.evaluate(test_set)